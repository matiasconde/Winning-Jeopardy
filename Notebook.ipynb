{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chisquare, chi2_contingency\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del jeopardy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Show Number    Air Date      Round                         Category  Value  \\\n",
      "0         4680  2004-12-31  Jeopardy!                          HISTORY   $200   \n",
      "1         4680  2004-12-31  Jeopardy!  ESPN's TOP 10 ALL-TIME ATHLETES   $200   \n",
      "2         4680  2004-12-31  Jeopardy!      EVERYBODY TALKS ABOUT IT...   $200   \n",
      "3         4680  2004-12-31  Jeopardy!                 THE COMPANY LINE   $200   \n",
      "4         4680  2004-12-31  Jeopardy!              EPITAPHS & TRIBUTES   $200   \n",
      "\n",
      "                                            Question      Answer  \n",
      "0  For the last 8 years of his life, Galileo was ...  Copernicus  \n",
      "1  No. 2: 1912 Olympian; football star at Carlisl...  Jim Thorpe  \n",
      "2  The city of Yuma in this state has a record av...     Arizona  \n",
      "3  In 1963, live on \"The Art Linkletter Show\", th...  McDonald's  \n",
      "4  Signer of the Dec. of Indep., framer of the Co...  John Adams  \n",
      "Index(['Show Number', ' Air Date', ' Round', ' Category', ' Value',\n",
      "       ' Question', ' Answer'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "jeopardy = pd.read_csv(\"jeopardy.csv\")\n",
    "print(jeopardy.head())\n",
    "print(jeopardy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jeopardy.columns = ['Show Number', 'Air Date', 'Round', 'Category', 'Value',\n",
    "       'Question', 'Answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    for the last 8 years of his life galileo was u...\n",
      "1    no 2 1912 olympian football star at carlisle i...\n",
      "2    the city of yuma in this state has a record av...\n",
      "3    in 1963 live on the art linkletter show this c...\n",
      "4    signer of the dec of indep framer of the const...\n",
      "Name: clean_question, dtype: object\n",
      "0    copernicus\n",
      "1    jim thorpe\n",
      "2       arizona\n",
      "3     mcdonalds\n",
      "4    john adams\n",
      "Name: clean_answer, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def normalize(s):\n",
    "    s = s.lower()\n",
    "    for punct in string.punctuation:\n",
    "        s = s.replace(punct,\"\")\n",
    "    return s\n",
    "\n",
    "jeopardy[\"clean_question\"] = jeopardy[\"Question\"].apply(normalize)\n",
    "jeopardy[\"clean_answer\"] = jeopardy[\"Answer\"].astype(str,copy=False).apply(normalize)\n",
    "print(jeopardy[\"clean_question\"].head())\n",
    "print(jeopardy[\"clean_answer\"].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_dollar(s):\n",
    "    s=s.replace(\"$\",\"\")\n",
    "    s=s.replace(\",\",\".\")\n",
    "    if s==\"None\":\n",
    "        return 0\n",
    "    return float(s)\n",
    "\n",
    "jeopardy[\"clean_value\"] = jeopardy[\"Value\"].apply(normalize_dollar)\n",
    "jeopardy[\"Air Date\"] = pd.to_datetime(jeopardy[\"Air Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         200.0\n",
       "1         200.0\n",
       "2         200.0\n",
       "3         200.0\n",
       "4         200.0\n",
       "5         200.0\n",
       "6         400.0\n",
       "7         400.0\n",
       "8         400.0\n",
       "9         400.0\n",
       "10        400.0\n",
       "11        400.0\n",
       "12        600.0\n",
       "13        600.0\n",
       "14        600.0\n",
       "15        600.0\n",
       "16        600.0\n",
       "17        600.0\n",
       "18        800.0\n",
       "19        800.0\n",
       "20        800.0\n",
       "21        800.0\n",
       "22          2.0\n",
       "23        800.0\n",
       "24       1000.0\n",
       "25       1000.0\n",
       "26       1000.0\n",
       "27       1000.0\n",
       "28       1000.0\n",
       "29        400.0\n",
       "          ...  \n",
       "19969    1200.0\n",
       "19970    1200.0\n",
       "19971       1.5\n",
       "19972    1200.0\n",
       "19973    1200.0\n",
       "19974    1200.0\n",
       "19975    1600.0\n",
       "19976    1600.0\n",
       "19977    1600.0\n",
       "19978    1600.0\n",
       "19979    1600.0\n",
       "19980    1600.0\n",
       "19981       1.2\n",
       "19982    2000.0\n",
       "19983    2000.0\n",
       "19984    2000.0\n",
       "19985    2000.0\n",
       "19986    2000.0\n",
       "19987       0.0\n",
       "19988     100.0\n",
       "19989     100.0\n",
       "19990     100.0\n",
       "19991     100.0\n",
       "19992     100.0\n",
       "19993     100.0\n",
       "19994     200.0\n",
       "19995     200.0\n",
       "19996     200.0\n",
       "19997     200.0\n",
       "19998     200.0\n",
       "Name: clean_value, Length: 19999, dtype: float64"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"clean_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prob_answer_in_question(row_jeopardy):\n",
    "# takes an answer, then divides it into words and finally verifies\n",
    "# the proportion of words in the answer also present in his\n",
    "# corresponding question\n",
    "   \n",
    "    split_answer = row_jeopardy[\"clean_answer\"].split(\" \")\n",
    "    split_question = row_jeopardy[\"clean_question\"].split(\" \")\n",
    "    \n",
    "    match_count = 0\n",
    "    if \"the\" in split_answer:\n",
    "        split_answer.remove(\"the\")\n",
    "    if len(split_answer)==0: return 0\n",
    "    for answer in split_answer:\n",
    "        if answer in split_question:\n",
    "            match_count += 1\n",
    "    return match_count / len(split_answer)\n",
    "\n",
    "jeopardy[\"answer_in_question\"] = jeopardy.apply(prob_answer_in_question,axis=1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06035277385469894"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"answer_in_question\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  First analysis\n",
    "The function created for the analysis takes each row of the data set \"hazard\" and count how many words of the answer appear in his question. then this amount is divided by the number of words in the answer:\n",
    "- This will tell us row by row what percentage of the answer appears in the question, 100% is that all the words of the answer appear in his question, 33.333% that only 1 of the three words of the answer appears in his question, etc. .\n",
    "\n",
    "Taking the average of all these values ​​generates a value that we can use to estimate the typical percentage of words in the answers that appear in his question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.687124288096678"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap = []\n",
    "terms_used = set()\n",
    "jeopardy.sort_values(by=\"Air Date\", inplace=True)\n",
    "\n",
    "for i,row in jeopardy.iterrows():\n",
    "    match_count = 0\n",
    "    split_question = row[\"clean_question\"].split(\" \")\n",
    "    split_question = [q for q in split_question if len(q) > 5]\n",
    "    for word in split_question:\n",
    "        if word in terms_used:\n",
    "            match_count += 1\n",
    "    for word in split_question:\n",
    "        terms_used.add(word)\n",
    "    if len(split_question)>0:\n",
    "        question_overlap.append(match_count / len(split_question))\n",
    "    else: question_overlap.append(0)\n",
    "\n",
    "jeopardy[\"question_overlap\"] = question_overlap\n",
    "jeopardy[\"question_overlap\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Conclusions: There is almost a 70% of isolated words that appears in old questions (for old questions we understand questions that occurred at most the immediate previous date). This value doesn't mean much since there are isolated words and not complete phrases . We need to investigate further.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def over_800USD(row):\n",
    "    if float(row[\"clean_value\"])>800:\n",
    "        value=1\n",
    "    else: value=0\n",
    "    return value\n",
    "\n",
    "jeopardy[\"high_value\"] = jeopardy.apply(over_800USD,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words(word):\n",
    "    low_count=0\n",
    "    high_count=0\n",
    "    for i,row in jeopardy.iterrows():\n",
    "        split_question = row[\"clean_question\"].split(\" \")\n",
    "        if word in split_question:\n",
    "            if row[\"high_value\"]==1:\n",
    "                high_count += 1\n",
    "            else: low_count += 1\n",
    "    return high_count, low_count\n",
    "\n",
    "observed_expected = []\n",
    "\n",
    "comparison_terms = list(terms_used)[:25]\n",
    "\n",
    "for term in comparison_terms:\n",
    "    result = count_words(term)\n",
    "    observed_expected.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (3, 3),\n",
       " (2, 1),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (0, 1),\n",
       " (1, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 2),\n",
       " (0, 1),\n",
       " (0, 2),\n",
       " (3, 8),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (2, 0),\n",
       " (1, 1),\n",
       " (3, 0),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (2, 1),\n",
       " (2, 0),\n",
       " (0, 3),\n",
       " (24, 86)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observed_expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "high_value_count = jeopardy[jeopardy[\"high_value\"]==1][\"high_value\"].count()\n",
    "low_value_count = jeopardy[jeopardy[\"high_value\"]==0][\"high_value\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3308710986890265, 0.565146603267378), (2.0297941782024855, 0.15424149441422405), (2.80672372637985, 0.09386990525628017), (0.3308710986890265, 0.565146603267378), (0.661742197378053, 0.4159455550913672), (0.3308710986890265, 0.565146603267378), (0.6765980594008285, 0.4107606373026974), (0.3308710986890265, 0.565146603267378), (3.022325020112631, 0.08212564786568953), (0.11526980495624546, 0.7342224981885828), (0.3308710986890265, 0.565146603267378), (0.661742197378053, 0.4159455550913672), (0.03424322701012426, 0.8531904022455312), (3.022325020112631, 0.08212564786568953), (3.022325020112631, 0.08212564786568953), (6.044650040225262, 0.013948497547915516), (0.6765980594008285, 0.4107606373026974), (9.066975060337892, 0.0026026726151395754), (0.3308710986890265, 0.565146603267378), (3.022325020112631, 0.08212564786568953), (3.022325020112631, 0.08212564786568953), (2.80672372637985, 0.09386990525628017), (6.044650040225262, 0.013948497547915516), (0.9926132960670793, 0.3191044998242515), (0.5452896135355879, 0.4602487686442909)]\n"
     ]
    }
   ],
   "source": [
    "chi_squared = []\n",
    "for each in observed_expected:\n",
    "    total = each[0]+each[1]\n",
    "    total_prop = total/jeopardy.shape[0]\n",
    "    expected_high_value_counts = total_prop*high_value_count\n",
    "    expected_low_value_counts = total_prop*low_value_count\n",
    "    expected = np.array([expected_high_value_counts,expected_low_value_counts])\n",
    "    observed = np.array([each[0],each[1]])\n",
    "    chisquare_value, pvalue = chisquare(observed, expected)\n",
    "    chi_squared.append((chisquare_value,pvalue))\n",
    "\n",
    "print(chi_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chis_squared_promediate:  2.010418596782989\n",
      "pvalues_promediate:  0.3277666954104502\n"
     ]
    }
   ],
   "source": [
    "chis, pval = 0, 0\n",
    "\n",
    "for each in chi_squared:\n",
    "    chis += each[0]\n",
    "    pval += each[1]\n",
    "\n",
    "chis_squared_promediate = chis/len(chi_squared)\n",
    "pvalues_promediate = pval/len(chi_squared)\n",
    "\n",
    "print(\"chis_squared_promediate: \",chis_squared_promediate)\n",
    "print(\"pvalues_promediate: \",pvalues_promediate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Observing p-values and chi-squared values:\n",
    "\n",
    "- Generlly we haven't low p-values indicating that the chi-squared value obtained  of the categorical distribution of words present in low and high USD values questions is not very rare. \n",
    "Then there is high probability of get this distribution by chance. Hence There isn't a Statistical significance in data for the study of \"high-low USD-value words\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some further analytis:\n",
    "- Increasing the list of english stopwords to remove from questions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "stopwords2 = ['a',\n",
    " 'about',\n",
    " 'above',\n",
    " 'after',\n",
    " 'again',\n",
    " 'against',\n",
    " 'all',\n",
    " 'am',\n",
    " 'an',\n",
    " 'and',\n",
    " 'any',\n",
    " 'are',\n",
    " \"aren't\",\n",
    " 'as',\n",
    " 'at',\n",
    " 'be',\n",
    " 'because',\n",
    " 'been',\n",
    " 'before',\n",
    " 'being',\n",
    " 'below',\n",
    " 'between',\n",
    " 'both',\n",
    " 'but',\n",
    " 'by',\n",
    " \"can't\",\n",
    " 'cannot',\n",
    " 'could',\n",
    " \"couldn't\",\n",
    " 'did',\n",
    " \"didn't\",\n",
    " 'do',\n",
    " 'does',\n",
    " \"doesn't\",\n",
    " 'doing',\n",
    " \"don't\",\n",
    " 'down',\n",
    " 'during',\n",
    " 'each',\n",
    " 'few',\n",
    " 'for',\n",
    " 'from',\n",
    " 'further',\n",
    " 'had',\n",
    " \"hadn't\",\n",
    " 'has',\n",
    " \"hasn't\",\n",
    " 'have',\n",
    " \"haven't\",\n",
    " 'having',\n",
    " 'he',\n",
    " \"he'd\",\n",
    " \"he'll\",\n",
    " \"he's\",\n",
    " 'her',\n",
    " 'here',\n",
    " \"here's\",\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'him',\n",
    " 'himself',\n",
    " 'his',\n",
    " 'how',\n",
    " \"how's\",\n",
    " 'i',\n",
    " \"i'd\",\n",
    " \"i'll\",\n",
    " \"i'm\",\n",
    " \"i've\",\n",
    " 'if',\n",
    " 'in',\n",
    " 'into',\n",
    " 'is',\n",
    " \"isn't\",\n",
    " 'it',\n",
    " \"it's\",\n",
    " 'its',\n",
    " 'itself',\n",
    " \"let's\",\n",
    " 'me',\n",
    " 'more',\n",
    " 'most',\n",
    " \"mustn't\",\n",
    " 'my',\n",
    " 'myself',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'of',\n",
    " 'off',\n",
    " 'on',\n",
    " 'once',\n",
    " 'only',\n",
    " 'or',\n",
    " 'other',\n",
    " 'ought',\n",
    " 'our',\n",
    " 'ours\\tourselves',\n",
    " 'out',\n",
    " 'over',\n",
    " 'own',\n",
    " 'same',\n",
    " \"shan't\",\n",
    " 'she',\n",
    " \"she'd\",\n",
    " \"she'll\",\n",
    " \"she's\",\n",
    " 'should',\n",
    " \"shouldn't\",\n",
    " 'so',\n",
    " 'some',\n",
    " 'such',\n",
    " 'than',\n",
    " 'that',\n",
    " \"that's\",\n",
    " 'the',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'them',\n",
    " 'themselves',\n",
    " 'then',\n",
    " 'there',\n",
    " \"there's\",\n",
    " 'these',\n",
    " 'they',\n",
    " \"they'd\",\n",
    " \"they'll\",\n",
    " \"they're\",\n",
    " \"they've\",\n",
    " 'this',\n",
    " 'those',\n",
    " 'through',\n",
    " 'to',\n",
    " 'too',\n",
    " 'under',\n",
    " 'until',\n",
    " 'up',\n",
    " 'very',\n",
    " 'was',\n",
    " \"wasn't\",\n",
    " 'we',\n",
    " \"we'd\",\n",
    " \"we'll\",\n",
    " \"we're\",\n",
    " \"we've\",\n",
    " 'were',\n",
    " \"weren't\",\n",
    " 'what',\n",
    " \"what's\",\n",
    " 'when',\n",
    " \"when's\",\n",
    " 'where',\n",
    " \"where's\",\n",
    " 'which',\n",
    " 'while',\n",
    " 'who',\n",
    " \"who's\",\n",
    " 'whom',\n",
    " 'why',\n",
    " \"why's\",\n",
    " 'with',\n",
    " \"won't\",\n",
    " 'would',\n",
    " \"wouldn't\",\n",
    " 'you',\n",
    " \"you'd\",\n",
    " \"you'll\",\n",
    " \"you're\",\n",
    " \"you've\",\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_overlap2</th>\n",
       "      <th>Air Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19325</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19274</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19275</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19276</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19277</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19278</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19279</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19280</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19281</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19282</th>\n",
       "      <td>0.200000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19283</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19284</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19286</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19287</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19288</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19289</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19290</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19291</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19292</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19293</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19294</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19295</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19296</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19298</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19297</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19299</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19285</th>\n",
       "      <td>0.125000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19300</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19324</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19301</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1919</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1945</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1930</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0.714286</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1925</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1924</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1931</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1920</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>0.833333</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>0.678571</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>0.866667</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1915</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1921</th>\n",
       "      <td>0.636364</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1916</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1918</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_overlap2   Air Date\n",
       "19325           0.000000 1984-09-21\n",
       "19274           0.000000 1984-09-21\n",
       "19275           0.000000 1984-09-21\n",
       "19276           0.000000 1984-09-21\n",
       "19277           0.000000 1984-09-21\n",
       "19278           0.000000 1984-09-21\n",
       "19279           0.000000 1984-09-21\n",
       "19280           0.000000 1984-09-21\n",
       "19281           0.000000 1984-09-21\n",
       "19282           0.200000 1984-09-21\n",
       "19283           0.000000 1984-09-21\n",
       "19284           0.000000 1984-09-21\n",
       "19286           0.166667 1984-09-21\n",
       "19287           0.000000 1984-09-21\n",
       "19288           0.000000 1984-09-21\n",
       "19289           0.333333 1984-09-21\n",
       "19290           0.100000 1984-09-21\n",
       "19291           0.000000 1984-09-21\n",
       "19292           0.000000 1984-09-21\n",
       "19293           0.000000 1984-09-21\n",
       "19294           0.111111 1984-09-21\n",
       "19295           0.000000 1984-09-21\n",
       "19296           0.111111 1984-09-21\n",
       "19298           0.111111 1984-09-21\n",
       "19297           0.100000 1984-09-21\n",
       "19299           0.000000 1984-09-21\n",
       "19285           0.125000 1984-09-21\n",
       "19300           0.166667 1984-09-21\n",
       "19324           0.000000 1984-09-21\n",
       "19301           0.166667 1984-09-21\n",
       "...                  ...        ...\n",
       "1919            1.000000 2012-01-19\n",
       "1945            1.000000 2012-01-19\n",
       "1930            0.666667 2012-01-19\n",
       "1929            0.714286 2012-01-19\n",
       "1928            0.800000 2012-01-19\n",
       "1927            1.000000 2012-01-19\n",
       "1926            0.800000 2012-01-19\n",
       "1925            0.750000 2012-01-19\n",
       "1924            1.000000 2012-01-19\n",
       "1923            0.666667 2012-01-19\n",
       "1931            1.000000 2012-01-19\n",
       "1920            1.000000 2012-01-19\n",
       "1933            0.900000 2012-01-19\n",
       "1960            0.750000 2012-01-19\n",
       "1961            1.000000 2012-01-19\n",
       "1962            1.000000 2012-01-19\n",
       "1964            0.833333 2012-01-19\n",
       "1965            0.678571 2012-01-19\n",
       "1966            0.800000 2012-01-19\n",
       "1967            1.000000 2012-01-19\n",
       "1968            1.000000 2012-01-19\n",
       "1969            0.866667 2012-01-19\n",
       "1970            1.000000 2012-01-19\n",
       "1915            0.750000 2012-01-19\n",
       "1921            0.636364 2012-01-19\n",
       "1916            1.000000 2012-01-19\n",
       "1917            1.000000 2012-01-19\n",
       "1918            0.888889 2012-01-19\n",
       "1971            1.000000 2012-01-19\n",
       "1922            0.888889 2012-01-19\n",
       "\n",
       "[19999 rows x 2 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap2 = []\n",
    "terms_used2 = set()\n",
    "jeopardy.sort_values(by=\"Air Date\", inplace=True)\n",
    "\n",
    "for i,row in jeopardy.iterrows():\n",
    "    match_count = 0\n",
    "    split_question = row[\"clean_question\"].split(\" \")\n",
    "    split_question = [q for q in split_question if (q not in stopwords2)] # and (len(q)>5)]\n",
    "    for word in split_question:\n",
    "        if word in terms_used2:\n",
    "            match_count += 1\n",
    "    for word in split_question:\n",
    "        terms_used2.add(word)\n",
    "    if len(split_question)>0:\n",
    "        question_overlap2.append(match_count / len(split_question))\n",
    "    else: question_overlap2.append(0)\n",
    "\n",
    "jeopardy[\"question_overlap2\"] = question_overlap2\n",
    "jeopardy[[\"question_overlap2\",\"Air Date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.796908142660021"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"question_overlap2\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New results: 79% of the words used in old questions appears on new ones. This time we do the cut with english stopwords rather than words with lenght < 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let´s go to obtain the frequency-value-words-appear more efficient using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24532"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19999, 14)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17859\n"
     ]
    }
   ],
   "source": [
    "terms_used_list = []\n",
    "for word in terms_used:\n",
    "    if len(word)<10:\n",
    "        terms_used_list.append(word)\n",
    "print(len(terms_used_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6034</th>\n",
       "      <td>engelbert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15843</th>\n",
       "      <td>ariane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7675</th>\n",
       "      <td>strikes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>surfacing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5184</th>\n",
       "      <td>heeeeres</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15577</th>\n",
       "      <td>holland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>langhama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14467</th>\n",
       "      <td>burned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17758</th>\n",
       "      <td>sealing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10574</th>\n",
       "      <td>hexagon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16032</th>\n",
       "      <td>slower</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>pahlavi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6285</th>\n",
       "      <td>kickoffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>allowing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11153</th>\n",
       "      <td>assyrian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13061</th>\n",
       "      <td>62mile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>knockoffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14728</th>\n",
       "      <td>scarlet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>diners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12776</th>\n",
       "      <td>arness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>anshan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15456</th>\n",
       "      <td>amendment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>monstrous</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>eyebrows</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16526</th>\n",
       "      <td>helmed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6338</th>\n",
       "      <td>vanessa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14767</th>\n",
       "      <td>partons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014</th>\n",
       "      <td>woodchuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>pledges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17049</th>\n",
       "      <td>charter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6414</th>\n",
       "      <td>dealer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11213</th>\n",
       "      <td>brothers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9497</th>\n",
       "      <td>presided</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>chavez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17643</th>\n",
       "      <td>virginias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7643</th>\n",
       "      <td>mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17005</th>\n",
       "      <td>starry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>satellite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>imitate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15654</th>\n",
       "      <td>elkhart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5229</th>\n",
       "      <td>mightily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>future</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12129</th>\n",
       "      <td>without</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10397</th>\n",
       "      <td>scenting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17520</th>\n",
       "      <td>clives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15047</th>\n",
       "      <td>larynx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9237</th>\n",
       "      <td>fantine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>throttle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11634</th>\n",
       "      <td>looking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503</th>\n",
       "      <td>strife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5992</th>\n",
       "      <td>cowpoke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>rossini</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>melanie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>insider</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16621</th>\n",
       "      <td>turgenevs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>trolley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17484</th>\n",
       "      <td>tornadoes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7646</th>\n",
       "      <td>philip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13850</th>\n",
       "      <td>buchanans</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>havliceks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word\n",
       "6034   engelbert\n",
       "15843     ariane\n",
       "7675     strikes\n",
       "2968   surfacing\n",
       "5184    heeeeres\n",
       "15577    holland\n",
       "10755   langhama\n",
       "14467     burned\n",
       "17758    sealing\n",
       "10574    hexagon\n",
       "16032     slower\n",
       "5454     pahlavi\n",
       "6285    kickoffs\n",
       "1917    allowing\n",
       "11153   assyrian\n",
       "13061     62mile\n",
       "4191   knockoffs\n",
       "14728    scarlet\n",
       "11507     diners\n",
       "12776     arness\n",
       "9595      anshan\n",
       "15456  amendment\n",
       "3449   monstrous\n",
       "586     eyebrows\n",
       "16526     helmed\n",
       "6338     vanessa\n",
       "14767    partons\n",
       "6014   woodchuck\n",
       "4788     pledges\n",
       "17049    charter\n",
       "6414      dealer\n",
       "11213   brothers\n",
       "9497    presided\n",
       "1104      chavez\n",
       "17643  virginias\n",
       "7643      mailer\n",
       "17005     starry\n",
       "2197   satellite\n",
       "3499     imitate\n",
       "15654    elkhart\n",
       "5229    mightily\n",
       "2512      future\n",
       "12129    without\n",
       "10397   scenting\n",
       "17520     clives\n",
       "15047     larynx\n",
       "9237     fantine\n",
       "7237    throttle\n",
       "11634    looking\n",
       "7503      strife\n",
       "5992     cowpoke\n",
       "4555     rossini\n",
       "8509     melanie\n",
       "10005    insider\n",
       "16621  turgenevs\n",
       "6432     trolley\n",
       "17484  tornadoes\n",
       "7646      philip\n",
       "13850  buchanans\n",
       "7497   havliceks"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_used_df = pd.DataFrame(terms_used_list,columns=[\"word\"]).sample(n=60)\n",
    "terms_used_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_words2(word):\n",
    "    low_count=0\n",
    "    high_count=0\n",
    "    for i,row in jeopardy.iterrows():\n",
    "        split_question = row[\"clean_question\"].split(\" \")\n",
    "        if word in split_question:\n",
    "            if row[\"high_value\"]==1:\n",
    "                high_count += 1\n",
    "            else: low_count += 1\n",
    "    return high_count, low_count, word\n",
    "\n",
    "terms_used_df[\"high-low value-word\"] = terms_used_df[\"word\"].apply(count_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>high-low value-word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6034</th>\n",
       "      <td>engelbert</td>\n",
       "      <td>(0, 1, engelbert)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15843</th>\n",
       "      <td>ariane</td>\n",
       "      <td>(1, 0, ariane)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7675</th>\n",
       "      <td>strikes</td>\n",
       "      <td>(2, 6, strikes)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2968</th>\n",
       "      <td>surfacing</td>\n",
       "      <td>(0, 1, surfacing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5184</th>\n",
       "      <td>heeeeres</td>\n",
       "      <td>(0, 1, heeeeres)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15577</th>\n",
       "      <td>holland</td>\n",
       "      <td>(2, 4, holland)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10755</th>\n",
       "      <td>langhama</td>\n",
       "      <td>(0, 1, langhama)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14467</th>\n",
       "      <td>burned</td>\n",
       "      <td>(4, 7, burned)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17758</th>\n",
       "      <td>sealing</td>\n",
       "      <td>(0, 1, sealing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10574</th>\n",
       "      <td>hexagon</td>\n",
       "      <td>(0, 1, hexagon)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16032</th>\n",
       "      <td>slower</td>\n",
       "      <td>(1, 0, slower)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5454</th>\n",
       "      <td>pahlavi</td>\n",
       "      <td>(1, 1, pahlavi)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6285</th>\n",
       "      <td>kickoffs</td>\n",
       "      <td>(0, 1, kickoffs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1917</th>\n",
       "      <td>allowing</td>\n",
       "      <td>(3, 3, allowing)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11153</th>\n",
       "      <td>assyrian</td>\n",
       "      <td>(2, 0, assyrian)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13061</th>\n",
       "      <td>62mile</td>\n",
       "      <td>(0, 1, 62mile)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4191</th>\n",
       "      <td>knockoffs</td>\n",
       "      <td>(1, 0, knockoffs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14728</th>\n",
       "      <td>scarlet</td>\n",
       "      <td>(1, 4, scarlet)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11507</th>\n",
       "      <td>diners</td>\n",
       "      <td>(0, 1, diners)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12776</th>\n",
       "      <td>arness</td>\n",
       "      <td>(0, 1, arness)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9595</th>\n",
       "      <td>anshan</td>\n",
       "      <td>(0, 1, anshan)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15456</th>\n",
       "      <td>amendment</td>\n",
       "      <td>(4, 19, amendment)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3449</th>\n",
       "      <td>monstrous</td>\n",
       "      <td>(0, 3, monstrous)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>eyebrows</td>\n",
       "      <td>(0, 2, eyebrows)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16526</th>\n",
       "      <td>helmed</td>\n",
       "      <td>(1, 0, helmed)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6338</th>\n",
       "      <td>vanessa</td>\n",
       "      <td>(0, 2, vanessa)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14767</th>\n",
       "      <td>partons</td>\n",
       "      <td>(0, 1, partons)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014</th>\n",
       "      <td>woodchuck</td>\n",
       "      <td>(0, 2, woodchuck)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4788</th>\n",
       "      <td>pledges</td>\n",
       "      <td>(0, 1, pledges)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17049</th>\n",
       "      <td>charter</td>\n",
       "      <td>(2, 4, charter)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6414</th>\n",
       "      <td>dealer</td>\n",
       "      <td>(4, 3, dealer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11213</th>\n",
       "      <td>brothers</td>\n",
       "      <td>(19, 32, brothers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9497</th>\n",
       "      <td>presided</td>\n",
       "      <td>(0, 3, presided)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1104</th>\n",
       "      <td>chavez</td>\n",
       "      <td>(1, 1, chavez)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17643</th>\n",
       "      <td>virginias</td>\n",
       "      <td>(3, 5, virginias)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7643</th>\n",
       "      <td>mailer</td>\n",
       "      <td>(0, 1, mailer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17005</th>\n",
       "      <td>starry</td>\n",
       "      <td>(0, 2, starry)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2197</th>\n",
       "      <td>satellite</td>\n",
       "      <td>(3, 8, satellite)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3499</th>\n",
       "      <td>imitate</td>\n",
       "      <td>(1, 4, imitate)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15654</th>\n",
       "      <td>elkhart</td>\n",
       "      <td>(0, 3, elkhart)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5229</th>\n",
       "      <td>mightily</td>\n",
       "      <td>(0, 1, mightily)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2512</th>\n",
       "      <td>future</td>\n",
       "      <td>(18, 45, future)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12129</th>\n",
       "      <td>without</td>\n",
       "      <td>(12, 59, without)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10397</th>\n",
       "      <td>scenting</td>\n",
       "      <td>(1, 0, scenting)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17520</th>\n",
       "      <td>clives</td>\n",
       "      <td>(1, 0, clives)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15047</th>\n",
       "      <td>larynx</td>\n",
       "      <td>(0, 2, larynx)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9237</th>\n",
       "      <td>fantine</td>\n",
       "      <td>(0, 1, fantine)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>throttle</td>\n",
       "      <td>(1, 1, throttle)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11634</th>\n",
       "      <td>looking</td>\n",
       "      <td>(6, 17, looking)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7503</th>\n",
       "      <td>strife</td>\n",
       "      <td>(0, 1, strife)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5992</th>\n",
       "      <td>cowpoke</td>\n",
       "      <td>(0, 1, cowpoke)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4555</th>\n",
       "      <td>rossini</td>\n",
       "      <td>(0, 3, rossini)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8509</th>\n",
       "      <td>melanie</td>\n",
       "      <td>(1, 2, melanie)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10005</th>\n",
       "      <td>insider</td>\n",
       "      <td>(1, 0, insider)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16621</th>\n",
       "      <td>turgenevs</td>\n",
       "      <td>(1, 0, turgenevs)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6432</th>\n",
       "      <td>trolley</td>\n",
       "      <td>(0, 2, trolley)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17484</th>\n",
       "      <td>tornadoes</td>\n",
       "      <td>(0, 2, tornadoes)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7646</th>\n",
       "      <td>philip</td>\n",
       "      <td>(4, 13, philip)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13850</th>\n",
       "      <td>buchanans</td>\n",
       "      <td>(0, 1, buchanans)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7497</th>\n",
       "      <td>havliceks</td>\n",
       "      <td>(0, 1, havliceks)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            word high-low value-word\n",
       "6034   engelbert   (0, 1, engelbert)\n",
       "15843     ariane      (1, 0, ariane)\n",
       "7675     strikes     (2, 6, strikes)\n",
       "2968   surfacing   (0, 1, surfacing)\n",
       "5184    heeeeres    (0, 1, heeeeres)\n",
       "15577    holland     (2, 4, holland)\n",
       "10755   langhama    (0, 1, langhama)\n",
       "14467     burned      (4, 7, burned)\n",
       "17758    sealing     (0, 1, sealing)\n",
       "10574    hexagon     (0, 1, hexagon)\n",
       "16032     slower      (1, 0, slower)\n",
       "5454     pahlavi     (1, 1, pahlavi)\n",
       "6285    kickoffs    (0, 1, kickoffs)\n",
       "1917    allowing    (3, 3, allowing)\n",
       "11153   assyrian    (2, 0, assyrian)\n",
       "13061     62mile      (0, 1, 62mile)\n",
       "4191   knockoffs   (1, 0, knockoffs)\n",
       "14728    scarlet     (1, 4, scarlet)\n",
       "11507     diners      (0, 1, diners)\n",
       "12776     arness      (0, 1, arness)\n",
       "9595      anshan      (0, 1, anshan)\n",
       "15456  amendment  (4, 19, amendment)\n",
       "3449   monstrous   (0, 3, monstrous)\n",
       "586     eyebrows    (0, 2, eyebrows)\n",
       "16526     helmed      (1, 0, helmed)\n",
       "6338     vanessa     (0, 2, vanessa)\n",
       "14767    partons     (0, 1, partons)\n",
       "6014   woodchuck   (0, 2, woodchuck)\n",
       "4788     pledges     (0, 1, pledges)\n",
       "17049    charter     (2, 4, charter)\n",
       "6414      dealer      (4, 3, dealer)\n",
       "11213   brothers  (19, 32, brothers)\n",
       "9497    presided    (0, 3, presided)\n",
       "1104      chavez      (1, 1, chavez)\n",
       "17643  virginias   (3, 5, virginias)\n",
       "7643      mailer      (0, 1, mailer)\n",
       "17005     starry      (0, 2, starry)\n",
       "2197   satellite   (3, 8, satellite)\n",
       "3499     imitate     (1, 4, imitate)\n",
       "15654    elkhart     (0, 3, elkhart)\n",
       "5229    mightily    (0, 1, mightily)\n",
       "2512      future    (18, 45, future)\n",
       "12129    without   (12, 59, without)\n",
       "10397   scenting    (1, 0, scenting)\n",
       "17520     clives      (1, 0, clives)\n",
       "15047     larynx      (0, 2, larynx)\n",
       "9237     fantine     (0, 1, fantine)\n",
       "7237    throttle    (1, 1, throttle)\n",
       "11634    looking    (6, 17, looking)\n",
       "7503      strife      (0, 1, strife)\n",
       "5992     cowpoke     (0, 1, cowpoke)\n",
       "4555     rossini     (0, 3, rossini)\n",
       "8509     melanie     (1, 2, melanie)\n",
       "10005    insider     (1, 0, insider)\n",
       "16621  turgenevs   (1, 0, turgenevs)\n",
       "6432     trolley     (0, 2, trolley)\n",
       "17484  tornadoes   (0, 2, tornadoes)\n",
       "7646      philip     (4, 13, philip)\n",
       "13850  buchanans   (0, 1, buchanans)\n",
       "7497   havliceks   (0, 1, havliceks)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "terms_used_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4972 15027\n"
     ]
    }
   ],
   "source": [
    "high_value_count2 = jeopardy[jeopardy[\"high_value\"]==1][\"high_value\"].count()\n",
    "low_value_count2 = jeopardy[jeopardy[\"high_value\"]==0][\"high_value\"].count()\n",
    "print(high_value_count2,low_value_count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.3308710986890265, 0.565146603267378, 'engelbert'), (3.022325020112631, 0.08212564786568953, 'ariane'), (8.245415693425013e-05, 0.9927549669483433, 'strikes'), (0.3308710986890265, 0.565146603267378, 'surfacing'), (0.3308710986890265, 0.565146603267378, 'heeeeres'), (0.23053960991249092, 0.63112400424852, 'holland'), (0.3308710986890265, 0.565146603267378, 'langhama'), (0.7790803779603984, 0.3774225405126984, 'burned'), (0.3308710986890265, 0.565146603267378, 'sealing'), (0.3308710986890265, 0.565146603267378, 'hexagon'), (3.022325020112631, 0.08212564786568953, 'slower'), (0.6765980594008285, 0.4107606373026974, 'pahlavi'), (0.3308710986890265, 0.565146603267378, 'kickoffs'), (2.0297941782024855, 0.15424149441422405, 'allowing'), (6.044650040225262, 0.013948497547915516, 'assyrian'), (0.3308710986890265, 0.565146603267378, '62mile'), (3.022325020112631, 0.08212564786568953, 'knockoffs'), (0.06325251982741063, 0.8014271475031749, 'scarlet'), (0.3308710986890265, 0.565146603267378, 'diners'), (0.3308710986890265, 0.565146603267378, 'arness'), (0.3308710986890265, 0.565146603267378, 'anshan'), (0.6870289977626362, 0.4071767708012848, 'amendment'), (0.9926132960670793, 0.3191044998242515, 'monstrous'), (0.661742197378053, 0.4159455550913672, 'eyebrows'), (3.022325020112631, 0.08212564786568953, 'helmed'), (0.661742197378053, 0.4159455550913672, 'vanessa'), (0.3308710986890265, 0.565146603267378, 'partons'), (0.661742197378053, 0.4159455550913672, 'woodchuck'), (0.3308710986890265, 0.565146603267378, 'pledges'), (0.23053960991249092, 0.63112400424852, 'charter'), (3.9050057442861905, 0.0481424629878943, 'dealer'), (4.193555633690641, 0.04057790600939577, 'brothers'), (0.9926132960670793, 0.3191044998242515, 'presided'), (0.6765980594008285, 0.4107606373026974, 'chavez'), (0.6840878310299169, 0.4081826382707563, 'virginias'), (0.3308710986890265, 0.565146603267378, 'mailer'), (0.661742197378053, 0.4159455550913672, 'starry'), (0.03424322701012426, 0.8531904022455312, 'satellite'), (0.06325251982741063, 0.8014271475031749, 'imitate'), (0.9926132960670793, 0.3191044998242515, 'elkhart'), (0.3308710986890265, 0.565146603267378, 'mightily'), (0.46424256129795, 0.4956478526722793, 'future'), (2.4081281328552087, 0.12070662388814722, 'without'), (3.022325020112631, 0.08212564786568953, 'scenting'), (3.022325020112631, 0.08212564786568953, 'clives'), (0.661742197378053, 0.4159455550913672, 'larynx'), (0.3308710986890265, 0.565146603267378, 'fantine'), (0.6765980594008285, 0.4107606373026974, 'throttle'), (0.018497749790580033, 0.8918161955026604, 'looking'), (0.3308710986890265, 0.565146603267378, 'strife'), (0.3308710986890265, 0.565146603267378, 'cowpoke'), (0.9926132960670793, 0.3191044998242515, 'rossini'), (0.11526980495624546, 0.7342224981885828, 'melanie'), (3.022325020112631, 0.08212564786568953, 'insider'), (3.022325020112631, 0.08212564786568953, 'turgenevs'), (0.661742197378053, 0.4159455550913672, 'trolley'), (0.661742197378053, 0.4159455550913672, 'tornadoes'), (0.016142117661621037, 0.898899531743258, 'philip'), (0.3308710986890265, 0.565146603267378, 'buchanans'), (0.3308710986890265, 0.565146603267378, 'havliceks')]\n"
     ]
    }
   ],
   "source": [
    "def chi_squared_df(term):\n",
    "    \n",
    "    total = term[0]+term[1]\n",
    "    total_prop = total/jeopardy.shape[0]\n",
    "    expected_high_value_counts = total_prop*high_value_count\n",
    "    expected_low_value_counts = total_prop*low_value_count\n",
    "    expected = np.array([expected_high_value_counts,expected_low_value_counts])\n",
    "    observed = np.array([term[0],term[1]])\n",
    "    chisquare_value, pvalue = chisquare(observed, expected)\n",
    "    chi_squared = (chisquare_value,pvalue,term[2])\n",
    "    return chi_squared\n",
    "\n",
    "chi_squared = terms_used_df[\"high-low value-word\"].apply(chi_squared_df)\n",
    "print(chi_squared.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ariane\n",
      "slower\n",
      "assyrian\n",
      "knockoffs\n",
      "helmed\n",
      "dealer\n",
      "brothers\n",
      "scenting\n",
      "clives\n",
      "insider\n",
      "turgenevs\n",
      "6034                   None\n",
      "15843        (1, 0, ariane)\n",
      "7675                   None\n",
      "2968                   None\n",
      "5184                   None\n",
      "15577                  None\n",
      "10755                  None\n",
      "14467                  None\n",
      "17758                  None\n",
      "10574                  None\n",
      "16032        (1, 0, slower)\n",
      "5454                   None\n",
      "6285                   None\n",
      "1917                   None\n",
      "11153      (2, 0, assyrian)\n",
      "13061                  None\n",
      "4191      (1, 0, knockoffs)\n",
      "14728                  None\n",
      "11507                  None\n",
      "12776                  None\n",
      "9595                   None\n",
      "15456                  None\n",
      "3449                   None\n",
      "586                    None\n",
      "16526        (1, 0, helmed)\n",
      "6338                   None\n",
      "14767                  None\n",
      "6014                   None\n",
      "4788                   None\n",
      "17049                  None\n",
      "6414         (4, 3, dealer)\n",
      "11213    (19, 32, brothers)\n",
      "9497                   None\n",
      "1104                   None\n",
      "17643                  None\n",
      "7643                   None\n",
      "17005                  None\n",
      "2197                   None\n",
      "3499                   None\n",
      "15654                  None\n",
      "5229                   None\n",
      "2512                   None\n",
      "12129                  None\n",
      "10397      (1, 0, scenting)\n",
      "17520        (1, 0, clives)\n",
      "15047                  None\n",
      "9237                   None\n",
      "7237                   None\n",
      "11634                  None\n",
      "7503                   None\n",
      "5992                   None\n",
      "4555                   None\n",
      "8509                   None\n",
      "10005       (1, 0, insider)\n",
      "16621     (1, 0, turgenevs)\n",
      "6432                   None\n",
      "17484                  None\n",
      "7646                   None\n",
      "13850                  None\n",
      "7497                   None\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "list_of_words = []\n",
    "for each in chi_squared.tolist():\n",
    "    \n",
    "    if each[1]<0.1:\n",
    "        word_significant = each[2]\n",
    "        print(word_significant)\n",
    "        list_of_words.append(word_significant)\n",
    "\n",
    "        \n",
    "def determining_low_pvalues(df):\n",
    "    \n",
    "    word = df[\"word\"]\n",
    "    if word in list_of_words:\n",
    "        return df[\"high-low value-word\"]\n",
    "\n",
    "print(terms_used_df.apply(determining_low_pvalues,axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### the chi-squared test shows than only a few words in a sample of 60 have statistical significance. The words are:\n",
    "- sparta\n",
    "- alfalfa\n",
    "- santini\n",
    "- denied\n",
    "- honcho\n",
    "- numeral\n",
    "- backcourt\n",
    "- dollpuss\n",
    "- tuxedos\n",
    "- strapped\n",
    "- predating\n",
    "\n",
    "All these words have appearances on high value questions (over 800 USD), the first one in the amount of appearances is \"sparta\" with 2.\n",
    "\n",
    "The numbers are not very high but we must to consider that we have only used a sample of n = 60 terms used in questions selected ramdomly. If we make a gross and possibly incorrect extrapolation, but only for the purpose of generating some ideas, 2 in 60 are the 3.33% and the whole set of terms used in older questions are approximatelly 17859 words, then the 3.33% of this number is 594 an insteresting amount of times that \"sparta\" appears on high-value questions from a total of 20,000 questions colected in the dataset.\n",
    "\n",
    "##### The next steps in all this complex analysis are the taking of a most bigger data-set (on external workplaces like google colab I have taked a data-set of 200,000 rows, the percentages of words used in older questions goes up as well as others but in these cases we need a most powerfull way to do computations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some last analysis: counting phrases overlaping on older questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_overlap3</th>\n",
       "      <th>Air Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19325</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19324</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19301</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19302</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19303</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19304</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19305</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19306</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19308</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19309</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19310</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19311</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19307</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19313</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19322</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19321</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19320</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19319</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19318</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19323</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19316</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19315</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19314</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19317</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19300</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19285</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19312</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19299</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19274</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1984-09-21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1942</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1937</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1940</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1939</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1938</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1936</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1934</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1935</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1941</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1944</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1946</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1922</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19999 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       question_overlap3   Air Date\n",
       "19325                0.0 1984-09-21\n",
       "19324                0.0 1984-09-21\n",
       "19301                0.0 1984-09-21\n",
       "19302                0.0 1984-09-21\n",
       "19303                0.0 1984-09-21\n",
       "19304                0.0 1984-09-21\n",
       "19305                0.0 1984-09-21\n",
       "19306                0.0 1984-09-21\n",
       "19308                0.0 1984-09-21\n",
       "19309                0.0 1984-09-21\n",
       "19310                0.0 1984-09-21\n",
       "19311                0.0 1984-09-21\n",
       "19307                0.0 1984-09-21\n",
       "19313                0.0 1984-09-21\n",
       "19322                0.0 1984-09-21\n",
       "19321                0.0 1984-09-21\n",
       "19320                0.0 1984-09-21\n",
       "19319                0.0 1984-09-21\n",
       "19318                0.0 1984-09-21\n",
       "19323                0.0 1984-09-21\n",
       "19316                0.0 1984-09-21\n",
       "19315                0.0 1984-09-21\n",
       "19314                0.0 1984-09-21\n",
       "19317                0.0 1984-09-21\n",
       "19300                0.0 1984-09-21\n",
       "19285                0.0 1984-09-21\n",
       "19312                0.0 1984-09-21\n",
       "19297                0.0 1984-09-21\n",
       "19299                0.0 1984-09-21\n",
       "19274                0.0 1984-09-21\n",
       "...                  ...        ...\n",
       "1942                 0.0 2012-01-19\n",
       "1971                 0.0 2012-01-19\n",
       "1937                 0.0 2012-01-19\n",
       "1940                 0.0 2012-01-19\n",
       "1939                 0.0 2012-01-19\n",
       "1938                 0.0 2012-01-19\n",
       "1936                 0.0 2012-01-19\n",
       "1974                 0.0 2012-01-19\n",
       "1934                 0.0 2012-01-19\n",
       "1956                 0.0 2012-01-19\n",
       "1957                 0.0 2012-01-19\n",
       "1958                 0.0 2012-01-19\n",
       "1959                 0.0 2012-01-19\n",
       "1935                 0.0 2012-01-19\n",
       "1941                 0.0 2012-01-19\n",
       "1955                 0.0 2012-01-19\n",
       "1932                 0.0 2012-01-19\n",
       "1944                 0.0 2012-01-19\n",
       "1946                 0.0 2012-01-19\n",
       "1947                 0.0 2012-01-19\n",
       "1948                 0.0 2012-01-19\n",
       "1949                 0.0 2012-01-19\n",
       "1950                 0.0 2012-01-19\n",
       "1951                 0.0 2012-01-19\n",
       "1952                 0.0 2012-01-19\n",
       "1953                 0.0 2012-01-19\n",
       "1954                 0.0 2012-01-19\n",
       "1973                 0.0 2012-01-19\n",
       "1972                 0.0 2012-01-19\n",
       "1922                 0.0 2012-01-19\n",
       "\n",
       "[19999 rows x 2 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_overlap3 = []\n",
    "phrases_used = set()\n",
    "jeopardy.sort_values(by=\"Air Date\", inplace=True)\n",
    "\n",
    "for i,row in jeopardy.iterrows():\n",
    "    match_count = 0\n",
    "    split_question = row[\"clean_question\"].split(\",\")\n",
    "    split_question = [q for q in split_question if (q not in stopwords2)] # and (len(q)>5)]\n",
    "    for phrase in split_question:\n",
    "        if phrase in phrases_used:\n",
    "            match_count += 1\n",
    "    for phrase in split_question:\n",
    "        phrases_used.add(phrase)\n",
    "    if len(split_question)>0:\n",
    "        question_overlap3.append(match_count / len(split_question))\n",
    "    else: question_overlap3.append(0)\n",
    "\n",
    "jeopardy[\"question_overlap3\"] = question_overlap3\n",
    "jeopardy[[\"question_overlap3\",\"Air Date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000600030001500075"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"question_overlap3\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The percentage are very very low, this can indicate that phrases have high varations over time. This doesn't mean that the subject of the questions has the same variations. Jeopardy can make small variances on questions to generate some aparent variability without varyng their instrinsict subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TELEVISION                      51\n",
       "U.S. GEOGRAPHY                  50\n",
       "LITERATURE                      45\n",
       "HISTORY                         40\n",
       "AMERICAN HISTORY                40\n",
       "BEFORE & AFTER                  40\n",
       "AUTHORS                         39\n",
       "WORD ORIGINS                    38\n",
       "WORLD CAPITALS                  37\n",
       "BODIES OF WATER                 36\n",
       "SPORTS                          36\n",
       "SCIENCE                         35\n",
       "SCIENCE & NATURE                35\n",
       "MAGAZINES                       35\n",
       "RHYME TIME                      35\n",
       "WORLD GEOGRAPHY                 33\n",
       "HISTORIC NAMES                  32\n",
       "WORLD HISTORY                   32\n",
       "ANNUAL EVENTS                   32\n",
       "IN THE DICTIONARY               31\n",
       "BIRDS                           31\n",
       "FICTIONAL CHARACTERS            31\n",
       "POTPOURRI                       30\n",
       "MEDICINE                        30\n",
       "OPERA                           30\n",
       "U.S. PRESIDENTS                 30\n",
       "TRAVEL & TOURISM                30\n",
       "ISLANDS                         30\n",
       "BALLET                          29\n",
       "ART                             28\n",
       "                                ..\n",
       "BRITISH NOVELS                   1\n",
       "SCOTTISH INVENTORS               1\n",
       "19TH CENTURY FICTION             1\n",
       "FROM MOVIE TO MUSICAL            1\n",
       "POLITICAL IDIOMS                 1\n",
       "CANDY                            1\n",
       "LABOR UNIONS                     1\n",
       "HISTORIC QUOTATIONS              1\n",
       "THE CABINET                      1\n",
       "NAMES IN THE NEWS                1\n",
       "KNOWLEDGE BY THE NUMBERS         1\n",
       "20th CENTURY AMERICANS           1\n",
       "SAINTHOOD                        1\n",
       "ROSE BOWL HISTORY                1\n",
       "U.S. RIVERS                      1\n",
       "AFRICA                           1\n",
       "THE 1970 TV SEASON               1\n",
       "2004                             1\n",
       "'60s OSCAR-WINNING FILMS         1\n",
       "COMIC BOOK HISTORY               1\n",
       "STRUCTURES                       1\n",
       "NAME'S THE SAME                  1\n",
       "THE MAP OF EUROPE                1\n",
       "19th CENTURY POLITICIANS         1\n",
       "SPORTS CITIES                    1\n",
       "HOBBIES                          1\n",
       "RIVERS                           1\n",
       "HAVE A CONTINENTAL BREAKFAST     1\n",
       "U.S. LANDMARKS                   1\n",
       "AWARDS & HONORS                  1\n",
       "Name: Category, Length: 3581, dtype: int64"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jeopardy[\"Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Looking the rankings in \"Categories\":\n",
    "- TELEVISION                        51\n",
    "- U.S. GEOGRAPHY                    50\n",
    "- LITERATURE                        45\n",
    "- BEFORE & AFTER                    40\n",
    "- AMERICAN HISTORY                  40\n",
    "- HISTORY                           40\n",
    "- AUTHORS                           39\n",
    "- WORD ORIGINS                      38\n",
    "- WORLD CAPITALS                    37\n",
    "- SPORTS                            36\n",
    "- BODIES OF WATER                   36\n",
    "- RHYME TIME                        35\n",
    "- SCIENCE                           35\n",
    "\n",
    "Based on this ranking of categories, we can surmise that the questions with the greatest variability fall on the categories most frequently ocurred. So, in order to have more chances of winning Jeopardy, the participant needs to have a vast knowledge in the top 5 categories, and a general culture for the rest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
